* 感知分类器

最简单形式的前馈神经网络，是一种二元线性分类器, 把矩阵上的输入  x  （实数值向量）映射到输出值  f(x) 上（一个二元的值）。
$$\displaystyle f(x)={\begin{cases}+1&{\text{if }}w\cdot x+b>0  \\
-1&{\text{else}}\end{cases}}$$ 
* 学习算法
 首先定义一些变量
+ x(j)  表示n维输入向量中的第j项
+ w(j)  表示权重向量的第j项
+ f(x)  表示神经元接受输入  x  产生的输出
+ α  是一个常数，符合  0<α≤1  （接受率）
更进一步，为了简便我们假定偏置量  b  等于0。因为一个额外的维度  n+1  维，可以用  x(n+1)=1 的形式加到输入向量，这样我们就可以用  w(n+1)  代替偏置量。

感知器的学习通过对所有训练实例进行多次的迭代进行更新的方式来建模。
令 $$\displaystyle D_{m}=\{(x_{1},y_{1}),\dots ,(x_{m},y_{m})\}$$ 表示一个有m个训练实例的训练集。
每次迭代权重向量以如下方式更新： 对于每个  $Dm={(x1,y1),…,(xm,ym)}$  中的每个  (x,y)  对，
$$ \displaystyle w(j):=w(j)+{\alpha (y-f(x))}{x(j)}\quad (j=1,\ldots ,n) $$

注意这意味着，仅当针对给定训练实例  (x,y)  产生的输出值  f(x)  与预期的输出值  y  不同时，权重向量才会发生改变。
如果存在一个正的常数  γ  和权重向量  w  ，对所有的  i  满足  y_i⋅(⟨w,x_i⟩+b)>γ  ，训练集  D_m  就被叫做线性分隔。 然而，如果训练集不是线性分隔的，那么这个算法则不能确保会收敛。


#+BEGIN_SRC python 
  import numpy as np


  class Perceptron(object):
      """
      Perceptron classifier.

      Parameters
      ------------
      eta : float
          Learning rate (between 0.0 and 1.0)
      n_iter : int
          Passes over the training dataset.

      Attributes
      -----------
      w_ : 1d-array
          Weights after fitting.
      errors_ : list
          Number of misclassifications in every epoch.

      """
    
      def __init__(self, eta=0.01, n_iter=10):
          self.eta = eta
          self.n_iter = n_iter    # the number of epochs

      def fit(self, X, y):
          """
          Fit training data.

          Parameters
          ----------
          X : {array-like}, shape = [n_samples, n_features]
              Training vectors, where n_samples is the number of samples and
              n_features is the number of features.
          y : array-like, shape = [n_samples]
              Target values.

          Returns
          -------
          self : object
          """
          self.w_ = np.zeros(1 + X.shape[1])    # wights, initialize 0
          self.errors_ = []

          for _ in range(self.n_iter):
              errors = 0
              for xi, target in zip(X, y):
                  update = self.eta * (target - self.predict(xi))  # learning rate * error
                  self.w_[1:] += update * xi
                  self.w_[0] += update
                  errors += int(update != 0.0)
              self.errors_.append(errors)
          return self

      def net_input(self, X):
          """
          Calculate net input w*x
          """
          return np.dot(X, self.w_[1:]) + self.w_[0]

      def predict(self, X):
          """
          Return class label after unit step.
          """
          return np.where(self.net_input(X) >= 0.0, 1, -1)
#+END_SRC

* 梯度上升法
梯度上升基于的思想是：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向搜寻。如果梯度记作 $\nabla$ ,则函数 $f(x,y)的梯度有下式表示：

$$ \nabla f(x,y)= \left(\begin{align} \frac{\partial f(x,y)}{\partial x} \\
                                      \frac{\partial f(x,y)}{\partial y}\end{align}\right)$$

这个梯度意味着沿x的方向移动 $\frac{\partial f(x,y)}{\partial x}$ , 沿y的方向移动 $\frac{\partial f(x,y)}{\partial y}$ 。

用向量来表示的话、梯度算法的迭代公式如下：

$$ w:= w +\alpha \nabla_w f(w)$$
其中
$\alpha$ 表示移动的步长。

该公式将一直被迭代执行，直至达到某个停止条件为止，比如迭代次数达到某个指定值或算法达到某个可以允许的误差范围。
