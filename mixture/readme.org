* GMM
** 《统计学习方法》引用
+ 高斯混合模型
高斯混合模型是指具有如下形式的概率分布模型：
$$ P(y | \theta) = \sum^K_{k=1} \alpha_k\phi (y | \theta_k)$$
其中， $\alpha_k$ 是系数， $\alpha_k \geq 0$, $\sum^K_{k=1}\alpha_k = 1$ ; $\phi(y |\theta_k)$ 是高斯分布密度， $\theta_k = (\mu_k, \sigma^2_k)$

$$\phi(y|\theta) = \frac{1}{\sqrt{2\pi}\sigma_k} \text{exp}\left(- \frac{(y -\mu_k)^2}{2\sigma^2_k}\right)$$
称为第k个分模型。

+ 高斯混合模型参数估计的EM算法
假设观察数据 $y_1,y_2,\cdots,y_N$ 由高斯混合模型生成，
$$P(y |\theta) = \sum^K{k=1}\alpha_k \phi(y|\theta)$$
其中，$\theta=(\alpha_1,\alpha_2,\cdots,\alpha_K; \theta_1,\theta_2,\cdots,\theta_K)$。 用EM算法估计高斯混合模型的参数 $\theta$ .

1. 明确隐变量，写出完全数据的对数似然函数
可以设想观测数据 $y_j$, j=1,2,...,N, 是这样产生的：首先依概率 $\alpha_k$ 选择第k个高斯分布模型 $\phi(y|\theta)$ ；
然后依第k个分模型的概率分布 $\phi(y|\theta)$ 生成观测数据 $y_j$ 。这是观测数据 $y_i$ , j=1,2,...,N, 是已知的； 反映观测数据
$y_i$ 来自第k个分模型的数据是未知的， k=1,2,...,K, 以隐变量 $\gamma_{jk}$ 表示，其定义如下：

$$\gamma_{jk} = \{ \begin{cases} 1 & 第j个观测来自第k个分模型 \\ 0 & 否则 \end{cases} \right }$$
j=1,2,...,N ; k=1,2,...,K;
$\gamma_{jk}$ 是 0-1 随机变量。

有了观测数据 $y_j$ 及未观测数据 $\gamma_{jk}$ , 那么完全数据是
$$(y_j, \gamma_{j1}, \gamma_{j2}, \cdots, \gamma_{jK})$$
j=1,2,...,N

于是，可以写出完全数据的对数的似然函数：
$$ P(y,\gamma | \theta) = \prod^N_{j=1} P(y_j, \gamma_{j1},\gamma_{j2},\cdots,\gamma_{jK} | \theta)$$
$$ = \prod^K_{k=1} \prod^N_{j=1} \left[ \alpha_k \phi(y_j | \theta_k)\right]^{\gamma_{jk}}$$
$$=\prod^{K}_{k=1}\alpha^{n_k}_k \prod^N_{j=1}\left[\phi(y_j | \theta_k)\right]^{\gamma_{jk}}$$


$$ = \prod^{K}_{k=1}\alpha^{n_k}_k \prod^{N}_{j=1} \left[ \frac{1}{\sqrt{2\pi}\sigma_k} \text{exp}(\frac{(y_j-\mu_k)^2}{2\sigma^2})\right]^{\gamma_{jk}}$$

其中，
$n_k= \sum^N_{j=1}\gamma_{jk}$, $\sum^K_{k=1}n_k= N$

那么，完全的数据对数似然函数为
$$ \text{log} P(y,\gamma |\theta) = \sum^K_{k=1} n_k \text{log} \alpha_k + 
\sum^N_{j=1}\gamma_{jk}\left[\text{log}(\frac{1}{\sqrt{2\pi}}) - \text{log}\sigma_k - \frac{1}{2\sigma^2_k}(y_j-\mu_k)^2 \right]$$

** log Gaussian PDF
高斯混合模型log似然函数估计
$\log N(x_i |\mu, \sigma)$ compute
#+BEGIN_SRC python :results output
  def log_gaussian_pdf(x_i, mu, sigma):
      """
      Compute log N(x_i | mu, sigma)
      """
      n = len(mu)
      a = n * np.log(2 * np.pi)
      _, b = np.linalg.slogdet(sigma)

      y = np.linalg.solve(sigma, x_i - mu)
      c = np.dot(x_i - mu, y)
      return -0.5 * (a + b + c)
#+END_SRC

** EM 维基百科引用
https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm
#+BEGIN_QUOTE wikipdia 
The EM algorithm is used to find (local) maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly. Typically these models involve latent variables in addition to unknown parameters and known data observations. That is, either missing values exist among the data, or the model can be formulated more simply by assuming the existence of further unobserved data points. For example, a mixture model can be described more simply by assuming that each observed data point has a corresponding unobserved data point, or latent variable, specifying the mixture component to which each data point belongs.

Finding a maximum likelihood solution typically requires taking the derivatives of the likelihood function with respect to all the unknown values, the parameters and the latent variables, and simultaneously solving the resulting equations. In statistical models with latent variables, this is usually impossible. Instead, the result is typically a set of interlocking equations in which the solution to the parameters requires the values of the latent variables and vice versa, but substituting one set of equations into the other produces an unsolvable equation.

The EM algorithm proceeds from the observation that there is a way to solve these two sets of equations numerically. One can simply pick arbitrary values for one of the two sets of unknowns, use them to estimate the second set, then use these new values to find a better estimate of the first set, and then keep alternating between the two until the resulting values both converge to fixed points. It's not obvious that this will work, but it can be proven that in this context it does, and that the derivative of the likelihood is (arbitrarily close to) zero at that point, which in turn means that the point is either a maximum or a saddle point.[13] In general, multiple maxima may occur, with no guarantee that the global maximum will be found. Some likelihoods also have singularities in them, i.e., nonsensical maxima. For example, one of the solutions that may be found by EM in a mixture model involves setting one of the components to have zero variance and the mean parameter for the same component to be equal to one of the data points.


#+END_QUOTE



* The Log Sum Exp Trick
ref from http://bayesjumping.net/log-sum-exp-trick/

In machine learning, arithmetic underflow can become a problem when multiplying together many small probabilities. 
In many models it can be useful to calculate the log sum of exponentials.

$$\log \sum_{i = 1}^n \exp (x_{i})$$


If $x_i$ is sufficiently large or small, this will result in an arithmetic overflow/underflow. To avoid this we can use 
a common trick called the Log Sum Exponential trick.

$$\begin{align} \log \sum_{i = 1}^n \exp (x_{i}) & = \log \exp(b) \sum_{i = 1}^n \exp (x_{i} - b) \\ & = b + \log \sum_{i = 1}^n \exp (x_{i} - b) \end{align}$$

where $b$ is max(x).

We can calculate this in Python with
#+BEGIN_SRC python :results output
  def logsumexp(log_probs, axis=None):
      _max = np.max(log_probs)
      ds = log_probs - _max
      exp_sum = np.exp(ds).sum(axis=axis)
      return _max + np.log(exp_sum)
#+END_SRC

or using SciPy
#+BEGIN_SRC python
from scipy.special import logsumexp
logsumexp(ns)
#+END_SRC





