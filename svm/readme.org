+ 拉格朗日乘数

#+BEGIN_QUOTE from Wikipedia
拉格朗日乘数是一种寻找多元函数在其变量受到一个或多个条件约束时的极值的方法。这种方法可以将一个有n个变量与k个约束条件的
最优化问题转换为一个解有n+k个变量的方程组的解的问题。这种方法引入了一个或一组新的未知数，即拉格朗日乘数，又称拉格朗日
乘子，或拉式乘子，它们是在转换后的方程，即越苏方程中作为梯度(gradient)的线性组合中各个向量的系数。

比如，要求f(x,y)在g(x,y)=c时的最大值时，我们可以引入新变量拉格朗日乘数 $\lambda$ ,这是我们只需要下列拉格朗日函数的极值：
$$\zeta(x,y,\lambda)=f(x,y) + \lambda \cdot (g(x,y) - c)$$
更一般地，对含n个变量和k个约束的情况，有：
$$\zeta(x_1, \dots , x_n, \lambda_1, \dots, \lambda_k) = f(x_, \dots, x_n) - \sum^k_{i=1}\lambda_i g_i(x_1, \dots, x_n)$$
#+END_QUOTE

+ 寻找最大间隔
如何求解数据集的最佳分隔直线？分隔超平面的形式可以写成 $w^T x + b$。要计算点A到分隔超平面的距离，就必须给出点到
分隔面的法线或垂线的长度。该值为 $$ |W^T A + b| / ||W||$$。这里的常数b类似于Logistic回归中的截距 $w_0$ 。这里的向量
w和常数b一起描述所给数据的分隔线或超平面。

+ 分类器求解的优化问题
要找出分类器定义中的w和b。为此，我们必须找出具有最小间隔的数据点，而这些数据点也就是支持向量。一旦找到具有最小
间隔的数据点，我们就需要对该间隔最大化。这就可以写作：
$$arg\max_{w,b} \{ min_{n} (label \cdot (w^T x + b)) \cdot \frac{1}{||w||} \}$$ 

直接求解上述问题相当苦难，所以我们将它转换称为另一种更容易求解的形式。首先考察以下上式中大括号内的部分。由于对
乘积进行优化是一件很讨厌的事情，因此我们要做的是固定其中一个因子而最大化其他因子。若果令所有支持向量的
label * (w^T x + b) 都为1，那么就可以通过求 $||w||^{-1}$ 的最大值来得到最终解。但是，并非所有数据点的
label *  (w^T x + b) 都等于1，只有那些离分隔超平面最近的点得到的值才为1.而离超平面越远的数据点，其
label * (w^T x + b) 的值也就越大。
在上述优化问题中，给定了一些约束条件然后求最优值，因此该问题是一个带约束条件的优化问题。这里约束条件就是
label * (w^T x + b) >= 1.0. 引入拉格朗日乘子法，基于约束套件来表述问题。由于这里约束的约束条件都是基于数据点的，
因此我们就可以将超平面写成数据点的形式。于是，优化目标函数最后可以写成：
$$ max_{\alpha} [ \sum^m_{i=1} \alpha - \frac{1}{2}\sum^m_{i,j=1} label^{(i)} \cdot label^{(j)} \cdot \alpha_i \cdot \alpha_j <x^{(i)}, x^{(j)}>]$$ 

其约束条件为：
$$ C\geq \alpha \geq 0, and, \sum^m_{i=1} \alpha_i \cdot label^{(i)} = 0 $$ 
这里的常数C用于控制“最大化间隔”和“保证大部分点的函数间隔小于1.0”这两个目标的权重。在优化算法的实现代码中，常数C是一个
参数，因此我们就可以通过调节该参数得到不同的结果。一旦求出了所有的 \alpha,那么分隔超平面就可以通过这些 \alpha 来表达。
这一结论十分直接，SVM中的主要工作就是求解这些 \alpha。

+ SMO 算法
1996年，John Platt 发布了一个称为SMO的强大算法，用于训练SVM。SMO表示序列最小优化(Sequential Minimal Optimization).
Platt的SMO算法是将大优化问题分解为多个小优化问题来求解的。这些小优化问题往往很容易求解，并且对它们进行顺序求解的结果与
将它们作为整体来求解的结果哦完全一致的。在结果完全相同的同时，SMO算法的求解时间短很多。
SMO算法的目标是求出一系列的alpha和b，一旦求出了这些alpha，就很容易计算出权重向量w并得到分隔超平面。
SMO算法的工作原理是：每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么就增大其中一个同时减小另一个。这里
所谓的“合适”就是指两个alpha必须要符合一定条件，条件之一就是这两个alpha必须要在间隔边界之外，而其第二个条件则是这两个
alpha还没进行过区间化处理或者不在边界上。

+ 利用核函数将数据映射到高维空间
SVM优化中一个特别好的地方就是，所有的运算都可以写成内积(inner product, 也成点积)的形式。向量的内积指的是两个向量相乘，之后
得到单个标量或数值。我们可以把内积运算替换乘核函数，而不必做简化处理。将内积替换成核函数的方式被称为核技巧(krnel trick)或者
核“变电”(kernel substation)。

+ 经向基核函数
经向基核函数是SVM中常用的一个核函数。经向基核函数是一个采用向量作为自变量的函数，能够基于向量距离运算输出一个标量。这个距离可以
是从<0,0>向量或者其他向量开始计算的距离。我们采用经向基核函数的高斯版本，其具体公式为：
$$ k(x,y) = exp(\frac{- ||x -y||^2}{2 \sigma^2})$$
其中，\sigma 是用户定义的用于确定到达率(reach)或者说函数值跌落到0的速度参数。



