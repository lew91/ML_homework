* Features engineering
Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.

* Sub-Problems of feature engineering
+ Feature Importance: An estimate of the usefulness of a feature
+ Feature Selection: From many features to a few that are useful
+ Feature Extraction: The automatic construction of new features from raw data
+ Feature Construction: The manual construction of new features from raw data

** Features extraction
Extracting features is one of the most critical task in data mining, and it generally affects your end result more
than the choice of data mining algorithm. Unfortunately, there are no hard and fast rule for choosing features that
will result in high-prefromance data mining. The choice of features determines the model that you are using to 
represent your data.

** Feature selection
After initial modeling, we will often have a large number of features to choose from, but we wish to select only a small subset.
+ *Reducing complexity*: Many data mining algorithms need significantly more time and resources when the number of features increase.
Reducing the number of features is a great way to make an algorithm run faster or with fewer resources.
+ *Reducing noise*: Adding extra features doesn't always lead to better performance. Extra feature may confuse the algorithm, finding
correlations and patterns in training data that do not have any actual meaning. This is  common in both smaller and larger datasets.
Choosing only appropriate features is a good way to reduce the chance of random correlations that have no real meaning.
+ *Creating readable models*: While many data mining algorithm will happily compute an answer for models with thousands of features, the
results may be difficult to interpret for a human. In these cases, it may be worth using fewer features and creating a model that a 
human can understand.

feature selection methods:
+ Wrapper methods evaluate multiple models using procedures that add and/or remove predictors to find the optimal combination that maximizes model performance. In essence, wrapper methods are search algorithms that treat the predictors as the inputs and utilize model performance as the output to be optimized.
+ Filter methods evaluate the relevance of the predictors outside of the predictive models and subsequently model only the predictors that pass some criterion. For example, for classification problems, each predictor could be individually evaluated to check if there is a plausible relationship between it and the observed classes. Only predictors with important relationships would then be included in a classification model


* Iterative process of feature engineering
+ Brainstorm features: Really get into the problem, look at a lot of data, study feature engineering on other problems and see what you can steal.
+ Devise features: Depends on your problem, but you may use automatic feature extraction, manual feature construction and mixtures of the two.
+ Select features: Use different feature importance scorings and feature selection methods to prepare one or more “views” for your models to operate upon.
+ Evaluate models: Estimate model accuracy on unseen data using the chosen features.


* Data preprocessing
** Dealing with missing data
using pandas model dealing with the missing data
DataFrame.isnull()
DataFrame.dropna()
DataFrame.dropna(axis=1)
DataFrame.dropna(how='all')
** Mapping ordinal features
#+BEGIN_SRC python
size_mapping = {
           'XL': 3,
           'L': 2,
           'M': 1}

df['size'] = df['size'].map(size_mapping)

# transform the integer values back to the original string
inv_size_mapping = {v: k for k, v in size_mapping.items()}
df['size'].map(inv_size_mapping)
#+END_SRC
** Performing one-hot encoding on nominal features
#+BEGIN_SRC python 
from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(categorical_features=[0], sparse=False)
#+END_SRC
** Bringing features onto the same scale
+ normalization: re-scaling to [0, 1], e.g. min-max scaling
+ standardization: more practical.
#+BEGIN_SRC python :results output
# min-max rescaling
from sklearn.preprocessing import MinMaxScaler

mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.transform(X_test)
#+END_SRC
#+BEGIN_SRC python :results output
# standarzation
from sklearn.preprocessing import StandardScaler

stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)
#+END_SRC

* Feature selection
** Univariate statistics
The simplest method to select features is using univariate statistics, that is by looking at each feature individually and running a statistical test to see whether it is related to the target.

Univariate statistics in sklearn:
+ for regression: f_regression
+ for classification: chi2 or f_classif 

Once we got the statistics and p-value, we have those choose:
+ SelectKBest: removes all but the k highest scoring features
+ SelectPercentile: removes all but a user-specified highest scoring percentage of features
+ GenericUnivariateSelect: allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator.
+ using common univariate statistical tests for each feature: false positive rate SelectFpr, false discovery rate SelectFdr, or family wise error SelectFwe.

** Recursive feature elimination
using common univariate statistical tests for each feature: false positive rate SelectFpr, false discovery rate SelectFdr, or family wise error SelectFwe.
#+BEGIN_SRC python :results output
from sklearn.feature_selection import RFE
from sklearn.svm import SVC

svc = SVC(kernel="linear", C=1)
rfe = RFE(estimator=svc, 
          n_features_to_select=6,  
          step=1)  
rfe.fit(X_train, y_train)

X_rfe_selected = rfe.transform(X_train)

mask = rfe.get_support()
print(mask)
#+END_SRC
** Feature selection using SelectFromModel
SelectFromModel is a meta-transformer that can be used along with any estimator that has a coef or feature_importances attribute after fitting. The features are considered unimportant and removed, if the corresponding coef or feature_importances values are below the provided threshold parameter. Apart from specifying the threshold numerically, there are build-in heuristics for finding a threshold using a string argument. Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”.

一些模型能比较每个 feature 的重要程度，例如 线性模型加上 L1 正则项之后不重要的特征的系数会惩罚为0，随机森林模型能计算每个 feature 的重要程度。
然后 sklearn 有个 SelectFromModel 函数可以配合这些模型进行特征选择

*L1-based feature selection*
L2 norm: $||w||^2_2 = \sum^m_{j=1} w^2_j$
L1 norm: $||w||_1 = \sum^m_{j=1} |w_j|$
与L2正则相比，L1正则会让更多系数为0
如果有个高维数据，有很多特征是无用，那么L1 regularizationn 就可以被当作一种特征选择方法。
#+BEGIN_SRC python 
  from sklearn.linear_model import LogisticRegression
  # sklearn 想用L1 正则，把penalty参数设为 ‘L1 ’
  lr = LogisticRegression(penalty='l1', C=0.1)
  lr.fit(X_train, y_train)

  print('Training accuracy:', lr.score(X_train, y_train))
  print('Test accuracy:', lr.score(X_test_std, y_test))
#+END_SRC 
随着 L1 正则项增大，无关特征便排除出模型 (系数变为 0)，因此 L1 正则可以作为特征选择的一种方法
结合 sklearn 的 SelectFromModel 进行选择
#+BEGIN_SRC python :results output
  from sklearn.feature_selection import SelectFromModel

  model_l1 = SelectFromModel(lr, threshold='median', prefit=True)
  X_l1_selected = model_l1.transform(X)
#+END_SRC







** Tree-based feature selection
随机森林算法可以测量各个特征的重要性，因此可以作为特征选择的一种手段
#+BEGIN_SRC python :results output
  from sklearn.ensemble import RandomForestClassifier

  feat_labels = df_wine.columns[1:]

  # 使用 decision tree 或 random forests 不需要 standardization或 normalization
  forest = RandomForestClassifier(n_estimators=1000,  
                                  random_state=0,
                                  n_jobs=-1)
  forest.fit(X_train, y_train)

  # random forest 比较特殊, 有 feature_importances 这个 attribute
  importances = forest.feature_importances_

  indices = np.argsort(importances)[::-1]
  for i, idx in enumerate(indices):
      print("%2d) %-*s %f" % (i + 1, 30, 
                              feat_labels[idx], 
                              importances[idx]))
#+END_SRC
结合 Sklearn 的 SelectFromModel 进行特征选择
#+BEGIN_SRC python :results output
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier

select_rf = SelectFromModel(forest, threshold=0.1, prefit=True)

# 或者重新训练一个模型
# select = SelectFromModel(RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1), threshold=0.15, prefit=True)
# select.fit(X_train, y_train)

X_train_rf = select_rf.transform(X_train)

print(X_train.shape[1])  # 原始特征维度
print(X_train_rf.shape[1])  # 特征选择后特征维度
#+END_SRC
也能将随机森林和 Sequential selection 结合起来
#+BEGIN_SRC python :results output
from sklearn.feature_selection import RFE
select = RFE(RandomForestClassifier(n_estimators=100, random_state=0), 
             n_features_to_select=3)

select.fit(X_train, y_train)
# visualize the selected features:
mask = select.get_support()
plt.matshow(mask.reshape(1, -1), cmap='gray_r');
#+END_SRC
* Feature extraction

