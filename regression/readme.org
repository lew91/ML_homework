#+TITLE: 运用回归模型进行房价预测


梯度下降法

taken from wikipedia
*梯度下降法* (Gradient descent) 是一个一阶最优化算法，通常也称为最速下降法。要使用梯度下降法找到一个函数的局部极小值，
必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离进行迭代。如果相仿地向梯度正方向迭代搜索，则会接近函数
的局部极大值，这个过程则被称为梯度上升法。

梯度下降方法基于以下的观察：如果实值函数 $F(x)$ 在点 $\alpha$ 处可微且有定义，那么函数 $F(x)$ 在 $\alpha$ 点沿着梯度相反的方向 
$-\nabla F(\alpha)$ 下降最快。因而，如果 $b=a - \gamma\nabla F(\alpha)$ , 对于 $\gamma > 0$ 为一个够小数值时成立，那么 $F(a) \geq F(b)$ .

其中， $\nabla$ 为梯度算子， 
$\nabla = (\frac{\partial}{\partial x_1}, \frac{\partial}{\partial x_2}, \ldots, \frac{\partial}{\partial x_n})^T$

损失函数
$$J(w) = \frac{1}{2} \sum^n_{i=1} (y^{(i)} - \hat y^{(i)})^2$$
更新规则
$$ w:=w-\eta\frac{\partial J}{\partial w}$$

#+BEGIN_SRC python :results output
  class LinearRegressionGD(object):
      def __init__(self, eta=0.001, n_iter=20):
          self.eta = eta #learning rate
          self.n_iter = n_iter

      def fit(self, X, y):
          self.coef_ = np.zeros(shape=(1, X.shape[1]))   #代表被训练的系数，初始为0
          self.intercept_ = np.zeros(1)
          self.cost_ = []

          for i in range(self.n_iter):
              output = self.net_input(X)
              errors = y - output
              self.coef_ += self.eta * np.dot(errors.T, X)
              self.intercept_ += self.eta * errors.sum()
              cost = (errors ** 2).sum()  / 2.0
              self.cost_.append(cost)
          return self

      def net_input(self, X):
          return np.dot(X, self.coef_.T) + self.intercept_

      def predict(self, X):
          return self.net_input(X)
#+END_SRC

[[./data/cost.png]] 
发现在epoch 5 之后cost基本就不能再缩小了



