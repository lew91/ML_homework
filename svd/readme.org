+ 奇异值分解(Singular Value Decomposition, SVD).
利用SVD实现，我们能够用小得多得数据集来表示原始数据集。这样做，实际上是去除了噪声和冗余信息。
当我们试图节省空间时，去除信息就是很崇高的目标了，但是在这里我们则是从数据中抽取信息。基于这个
视角，我们就可以把SVD看成是从噪声数据中抽取相关特征。

+ Latent semantic indexing
最早的SVD应用之一就是信息检索。我们称利用SVD的方法为隐性语义索引(Latent Semaantic Indexing,
LSI)或隐性语义分析(Latent Semantic Analysis, LSA)。
在LSI中，一个矩阵是由文档和词语组成的。当我们在该矩阵上应用SVD时，就会构建出多个奇异值。这些
奇异值代表了文档中的概念或主题。这以特点可以用于更高效的文档搜索。在词语拼写错误时，只基于词语
存在与否的简单搜索方法会遇到问题。简单搜索的另一个问题另一个问题就是同义词的使用。这就是说，
当我们查找一个词时，其同义词所在的文档可能并不会匹配上。如果我们从上千篇相似的文档中抽取出概念，
那么同义词就会映射为同一概念。

+ Recommendation systems
SVD的另一个应用就是推荐系统。简单版本的推荐系统能够计算项或者人之间的相似度。更先进的方法则
先利用SVD从数据中构建一个主题空间，然后再在该空间下计算其相似度。

+ 矩阵分解
不同的矩阵分解计数具有不同的性质，其中有些更适合于某个应用，有些则更适合于其他应用。最常见的
一种矩阵分解技术就是SVD。SVD将原始的数据矩阵Data分解成三个矩阵U、 \Sigma 、和V^T . 如果原始矩阵
Data是m行n列，那么U、 \Sigma 和V^T 就分别是m行m列、m行n列和n行n列。为了清晰起见，上述过程可以写
成如下一行（下标为矩阵维数）：
$$Data_{m \times n} = U_{m \times m} \Sigma_{m \times n} V^T_{n \times n}$$ 
上述分解中会构建出一个矩阵 $\Sigma$ ，该矩阵只有对角元素，其他元素均为0。另一个惯例就是，$\Sigma$
的对角元素是从大到小排列的。这些对角元素称为奇异值(Singular Value),它们对应了原始数据集矩阵Data的
奇异值。奇异值和特征值是有关系的。这里的奇异值就是矩阵 Data * Data^T 特征值的平方根。
前面提到过，矩阵 $\Sigma$只有从大到小排列的对角元素。在科学和工程中，一直存在这样一个普遍事实：在
某个奇异值的数目（r个）之后，其他的奇异值都置为0.这就意味着数据集中仅有r个重要特征，而其余特征则都是
噪声或冗余特征。
